{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86416523-915a-41e6-afd2-c0fa12efa40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUANTUM HACKATHON: MOLECULAR CLASSIFICATION (3 Datasets)\n",
      "======================================================================\n",
      "\n",
      "[1] LOADING DATASETS...\n",
      "✓ Dataset MUTAG loaded successfully\n",
      "  - Number of graphs: 188\n",
      "  - Number of classes: 2\n",
      "  - Number of node features: 7\n",
      "✓ Dataset PTC_MR loaded successfully\n",
      "  - Number of graphs: 344\n",
      "  - Number of classes: 2\n",
      "  - Number of node features: 18\n",
      "✓ Dataset AIDS loaded successfully\n",
      "  - Number of graphs: 2000\n",
      "  - Number of classes: 2\n",
      "  - Number of node features: 38\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: MUTAG\n",
      "======================================================================\n",
      "Converting 188 PyG graphs to NetworkX...\n",
      "Total samples: 188\n",
      "Class distribution: [ 63 125]\n",
      "Fold 1/10: Test Acc = 0.789\n",
      "Fold 2/10: Test Acc = 0.789\n",
      "Fold 3/10: Test Acc = 0.789\n",
      "Fold 4/10: Test Acc = 0.895\n",
      "Fold 5/10: Test Acc = 0.947\n",
      "Fold 6/10: Test Acc = 0.789\n",
      "Fold 7/10: Test Acc = 0.684\n",
      "Fold 8/10: Test Acc = 0.895\n",
      "Fold 9/10: Test Acc = 0.944\n",
      "Fold 10/10: Test Acc = 0.722\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: PTC_MR\n",
      "======================================================================\n",
      "Converting 344 PyG graphs to NetworkX...\n",
      "Total samples: 344\n",
      "Class distribution: [192 152]\n",
      "Fold 1/10: Test Acc = 0.600\n",
      "Fold 2/10: Test Acc = 0.743\n",
      "Fold 3/10: Test Acc = 0.629\n",
      "Fold 4/10: Test Acc = 0.571\n",
      "Fold 5/10: Test Acc = 0.529\n",
      "Fold 6/10: Test Acc = 0.588\n",
      "Fold 7/10: Test Acc = 0.529\n",
      "Fold 8/10: Test Acc = 0.647\n",
      "Fold 9/10: Test Acc = 0.559\n",
      "Fold 10/10: Test Acc = 0.471\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: AIDS\n",
      "======================================================================\n",
      "Converting 2000 PyG graphs to NetworkX...\n",
      "Total samples: 2000\n",
      "Class distribution: [ 400 1600]\n",
      "Fold 1/10: Test Acc = 0.965\n",
      "Fold 2/10: Test Acc = 0.955\n",
      "Fold 3/10: Test Acc = 0.950\n",
      "Fold 4/10: Test Acc = 0.955\n",
      "Fold 5/10: Test Acc = 0.975\n",
      "Fold 6/10: Test Acc = 0.975\n",
      "Fold 7/10: Test Acc = 0.975\n",
      "Fold 8/10: Test Acc = 0.950\n",
      "Fold 9/10: Test Acc = 0.960\n",
      "Fold 10/10: Test Acc = 0.975\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Dataset         Mean Test Acc        Best C     Best Gamma\n",
      "----------------------------------------------------------------------\n",
      "MUTAG           0.825 ± 0.086         10         0.001\n",
      "PTC_MR          0.587 ± 0.071         10         scale\n",
      "AIDS            0.964 ± 0.010         100        scale\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings\n",
    "\n",
    "class MolecularGraphFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Integrated molecular graph feature extractor combining:\n",
    "    - Chemical Laplacian (global chemical properties)\n",
    "    - WL Histograms (local substructure patterns)\n",
    "    - Topological features (graph structure)\n",
    "    - Bridge tree features (hierarchical structure)\n",
    "    \"\"\"\n",
    "\n",
    "    # Expanded atom type weights (α parameter) - Based on electronegativity and atomic properties\n",
    "    ATOM_WEIGHTS = {\n",
    "        # Common organic atoms\n",
    "        'H': 0.5,   'C': 1.0,   'N': 1.5,   'O': 2.0,   'S': 2.5,   'P': 2.1,\n",
    "        'B': 1.4,   'Si': 1.3,  'Se': 2.4,  'As': 2.3,  'Sb': 2.0,  'Te': 2.1,\n",
    "        'Ge': 1.8,  \n",
    "        # Halogens\n",
    "        'F': 1.8,   'Cl': 1.8,  'Br': 2.0,  'I': 2.2,\n",
    "        # Alkali metals\n",
    "        'Li': 1.0,  'Na': 1.2,  'K': 1.1,\n",
    "        # Alkaline earth metals\n",
    "        'Mg': 1.3,  'Ca': 1.2,  'Ba': 1.1,\n",
    "        # Transition metals\n",
    "        'Cu': 1.9,  'Zn': 1.6,  'Co': 1.8,  'Ni': 1.9,  'Pd': 2.2,  'Pt': 2.3,\n",
    "        'Au': 2.4,  'Ru': 2.2,  'Rh': 2.3,  \n",
    "        # Post-transition metals\n",
    "        'Sn': 1.7,  'Pb': 1.9,  'In': 1.7,  'Tl': 1.8,  'Ga': 1.6,  'Bi': 1.9,\n",
    "        # Lanthanides/Actinides\n",
    "        'Ho': 1.2,  'Tb': 1.2,\n",
    "        # Others\n",
    "        'W': 2.4,   'Hg': 2.0,\n",
    "    }\n",
    "\n",
    "    def __init__(self, n_eigen=10, wl_iterations=3, dataset_name=None):\n",
    "        self.n_eigen = n_eigen\n",
    "        self.wl_iterations = wl_iterations\n",
    "        self.dataset_name = dataset_name\n",
    "        self._set_dataset_mappings()\n",
    "\n",
    "    def _set_dataset_mappings(self):\n",
    "        \"\"\"Set atom and bond type mappings based on dataset\"\"\"\n",
    "        if self.dataset_name == \"MUTAG\":\n",
    "            self.ATOM_TYPES = {0: 'C', 1: 'N', 2: 'O', 3: 'F', 4: 'I', 5: 'Cl', 6: 'Br'}\n",
    "            self.BOND_TYPES = {0: 1.5, 1: 1.0, 2: 2.0, 3: 3.0}  # aromatic, single, double, triple\n",
    "            \n",
    "        elif self.dataset_name == \"PTC_MR\":\n",
    "            self.ATOM_TYPES = {\n",
    "                0: 'In', 1: 'P', 2: 'O', 3: 'N', 4: 'Na', 5: 'C', 6: 'Cl', 7: 'S',\n",
    "                8: 'Br', 9: 'F', 10: 'K', 11: 'Cu', 12: 'Zn', 13: 'I', 14: 'Ba',\n",
    "                15: 'Sn', 16: 'Pb', 17: 'Ca'\n",
    "            }\n",
    "            self.BOND_TYPES = {0: 3.0, 1: 2.0, 2: 1.0, 3: 1.5}  # triple, double, single, aromatic\n",
    "            \n",
    "        elif self.dataset_name == \"AIDS\":\n",
    "            self.ATOM_TYPES = {\n",
    "                0: 'C', 1: 'O', 2: 'N', 3: 'Cl', 4: 'F', 5: 'S', 6: 'Se', 7: 'P',\n",
    "                8: 'Na', 9: 'I', 10: 'Co', 11: 'Br', 12: 'Li', 13: 'Si', 14: 'Mg',\n",
    "                15: 'Cu', 16: 'As', 17: 'B', 18: 'Pt', 19: 'Ru', 20: 'K', 21: 'Pd',\n",
    "                22: 'Au', 23: 'Te', 24: 'W', 25: 'Rh', 26: 'Zn', 27: 'Bi', 28: 'Pb',\n",
    "                29: 'Ge', 30: 'Sb', 31: 'Sn', 32: 'Ga', 33: 'Hg', 34: 'Ho', 35: 'Tl',\n",
    "                36: 'Ni', 37: 'Tb'\n",
    "            }\n",
    "            self.BOND_TYPES = {0: 1.5, 1: 1.0, 2: 2.0, 3: 3.0}  # aromatic, single, double, triple\n",
    "        else:\n",
    "            # Default mappings\n",
    "            self.ATOM_TYPES = {0: 'C', 1: 'N', 2: 'O', 3: 'F', 4: 'I', 5: 'Cl', 6: 'Br'}\n",
    "            self.BOND_TYPES = {0: 1.5, 1: 1.0, 2: 2.0, 3: 3.0}\n",
    "\n",
    "    # ========== CHEMICAL LAPLACIAN METHODS ==========\n",
    "\n",
    "    def _build_chemical_laplacian(self, G, node_labels, edge_labels):\n",
    "        \"\"\"\n",
    "        Build integrated chemical Laplacian:\n",
    "        L_chem(i,j) = {\n",
    "            α(v_i) · Σw(i,k)           if i = j\n",
    "            -γ(w(i,j)) · w(i,j)        if (i,j) ∈ E\n",
    "            0                           otherwise\n",
    "        }\n",
    "        \"\"\"\n",
    "        n = G.number_of_nodes()\n",
    "        node_list = list(G.nodes())\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(node_list)}\n",
    "\n",
    "        L = np.zeros((n, n))\n",
    "\n",
    "        if edge_labels is None:\n",
    "            edge_labels = {edge: 1.0 for edge in G.edges()}\n",
    "\n",
    "        # Diagonal: α(v_i) · Σw(i,k)\n",
    "        for node in node_list:\n",
    "            i = node_to_idx[node]\n",
    "            atom_type = node_labels.get(node, 'C')\n",
    "            alpha = self.ATOM_WEIGHTS.get(atom_type, 1.0)\n",
    "\n",
    "            weighted_degree = 0.0\n",
    "            for neighbor in G.neighbors(node):\n",
    "                edge = (node, neighbor) if (node, neighbor) in edge_labels else (neighbor, node)\n",
    "                bond_order = edge_labels.get(edge, 1.0)\n",
    "                weighted_degree += bond_order\n",
    "\n",
    "            L[i, i] = alpha * weighted_degree\n",
    "\n",
    "        # Off-diagonal: -γ(w(i,j)) · w(i,j)\n",
    "        for u, v in G.edges():\n",
    "            i, j = node_to_idx[u], node_to_idx[v]\n",
    "            edge = (u, v) if (u, v) in edge_labels else (v, u)\n",
    "            bond_order = edge_labels.get(edge, 1.0)\n",
    "            gamma = 1.2 if abs(bond_order - 1.5) < 0.01 else 1.0\n",
    "\n",
    "            weight = gamma * bond_order\n",
    "            L[i, j] = -weight\n",
    "            L[j, i] = -weight\n",
    "\n",
    "        return L\n",
    "\n",
    "    def _extract_chemical_laplacian_features(self, G, node_labels, edge_labels):\n",
    "        \"\"\"Extract spectral features from Chemical Laplacian\"\"\"\n",
    "        try:\n",
    "            n = G.number_of_nodes()\n",
    "\n",
    "            if n <= 2:\n",
    "                return np.zeros(self.n_eigen + 2)\n",
    "\n",
    "            L_chem = self._build_chemical_laplacian(G, node_labels, edge_labels)\n",
    "\n",
    "            k = min(self.n_eigen, n - 1)\n",
    "            L_sparse = csr_matrix(L_chem)\n",
    "            eigenvalues = eigsh(L_sparse, k=k, which='SM', return_eigenvectors=False)\n",
    "            eigenvalues = np.sort(eigenvalues)\n",
    "\n",
    "            if len(eigenvalues) < self.n_eigen:\n",
    "                eigenvalues = np.concatenate([\n",
    "                    eigenvalues,\n",
    "                    np.zeros(self.n_eigen - len(eigenvalues))\n",
    "                ])\n",
    "\n",
    "            eigenvalues_norm = eigenvalues[:self.n_eigen] / (n + 1)\n",
    "\n",
    "            # Additional spectral features\n",
    "            trace_norm = np.trace(L_chem) / (n * n)\n",
    "            algebraic_conn = eigenvalues[1] / (n + 1) if len(eigenvalues) > 1 else 0\n",
    "\n",
    "            return np.concatenate([eigenvalues_norm, [trace_norm, algebraic_conn]])\n",
    "\n",
    "        except:\n",
    "            return np.zeros(self.n_eigen + 2)\n",
    "\n",
    "    # ========== WEISFEILER-LEHMAN METHODS ==========\n",
    "\n",
    "    def _generate_wl_signatures(self, G):\n",
    "        \"\"\"Generate WL signatures across all iterations\"\"\"\n",
    "        all_signatures_per_iteration = [set() for _ in range(self.wl_iterations + 1)]\n",
    "\n",
    "        labels = {node: G.nodes[node]['atom_type'] for node in G.nodes()}\n",
    "        for label in labels.values():\n",
    "            all_signatures_per_iteration[0].add(str(label))\n",
    "\n",
    "        for iteration in range(self.wl_iterations):\n",
    "            new_labels = {}\n",
    "            for node in G.nodes():\n",
    "                neighbor_labels = sorted([labels[neighbor] for neighbor in G.neighbors(node)])\n",
    "                signature = str(labels[node]) + '|' + '|'.join(map(str, neighbor_labels))\n",
    "                all_signatures_per_iteration[iteration + 1].add(signature)\n",
    "                new_labels[node] = signature\n",
    "            labels = new_labels\n",
    "\n",
    "        return all_signatures_per_iteration\n",
    "\n",
    "    def _compute_wl_histogram(self, G):\n",
    "        \"\"\"Compute WL histogram using learned vocabulary\"\"\"\n",
    "        all_histograms = []\n",
    "        labels = {node: str(G.nodes[node]['atom_type']) for node in G.nodes()}\n",
    "\n",
    "        for iteration in range(self.wl_iterations + 1):\n",
    "            vocab_size = self.vocab_size_per_iteration_[iteration]\n",
    "            histogram = np.zeros(vocab_size, dtype=np.float32)\n",
    "\n",
    "            for signature in labels.values():\n",
    "                if signature in self.vocab_per_iteration_[iteration]:\n",
    "                    idx = self.vocab_per_iteration_[iteration][signature]\n",
    "                    histogram[idx] += 1\n",
    "\n",
    "            all_histograms.extend(histogram)\n",
    "\n",
    "            if iteration < self.wl_iterations:\n",
    "                new_labels = {}\n",
    "                for node in G.nodes():\n",
    "                    neighbor_labels = sorted([labels[neighbor] for neighbor in G.neighbors(node)])\n",
    "                    signature = labels[node] + '|' + '|'.join(neighbor_labels)\n",
    "                    new_labels[node] = signature\n",
    "                labels = new_labels\n",
    "\n",
    "        return np.array(all_histograms)\n",
    "\n",
    "    # ========== TOPOLOGICAL FEATURES ==========\n",
    "\n",
    "    def _extract_topological_features(self, G):\n",
    "        \"\"\"Extract topology-based features\"\"\"\n",
    "        features = []\n",
    "\n",
    "        features.append(G.number_of_nodes())\n",
    "        features.append(G.number_of_edges())\n",
    "        features.append(nx.density(G))\n",
    "\n",
    "        degrees = [d for n, d in G.degree()]\n",
    "        if degrees:\n",
    "            features.extend([np.mean(degrees), np.std(degrees),\n",
    "                           np.max(degrees), np.min(degrees)])\n",
    "        else:\n",
    "            features.extend([0, 0, 0, 0])\n",
    "\n",
    "        try:\n",
    "            features.append(nx.average_clustering(G))\n",
    "        except:\n",
    "            features.append(0)\n",
    "\n",
    "        features.append(nx.number_connected_components(G))\n",
    "\n",
    "        triangles = sum(nx.triangles(G).values()) / 3\n",
    "        features.append(triangles)\n",
    "\n",
    "        try:\n",
    "            if nx.is_connected(G):\n",
    "                features.append(nx.diameter(G))\n",
    "                features.append(nx.average_shortest_path_length(G))\n",
    "            else:\n",
    "                features.append(0)\n",
    "                features.append(0)\n",
    "        except:\n",
    "            features.append(0)\n",
    "            features.append(0)\n",
    "\n",
    "        if degrees:\n",
    "            features.append(np.percentile(degrees, 25))\n",
    "            features.append(np.percentile(degrees, 75))\n",
    "        else:\n",
    "            features.extend([0, 0])\n",
    "\n",
    "        return np.array(features)\n",
    "\n",
    "    def _extract_bridge_tree_features(self, G):\n",
    "        \"\"\"Extract features from bridge tree\"\"\"\n",
    "        try:\n",
    "            bridges = set(nx.bridges(G))\n",
    "            non_bridge_edges = [e for e in G.edges()\n",
    "                              if e not in bridges and (e[1], e[0]) not in bridges]\n",
    "\n",
    "            H = nx.Graph()\n",
    "            H.add_edges_from(non_bridge_edges)\n",
    "\n",
    "            comp_map = {}\n",
    "            for i, comp in enumerate(nx.connected_components(H)):\n",
    "                for node in comp:\n",
    "                    comp_map[node] = i\n",
    "\n",
    "            BT = nx.Graph()\n",
    "            BT.add_nodes_from(set(comp_map.values()))\n",
    "\n",
    "            for u, v in bridges:\n",
    "                if u in comp_map and v in comp_map:\n",
    "                    BT.add_edge(comp_map[u], comp_map[v])\n",
    "\n",
    "            return self._extract_topological_features(BT)\n",
    "        except:\n",
    "            return np.zeros(15)\n",
    "\n",
    "    # ========== FIT/TRANSFORM INTERFACE ==========\n",
    "\n",
    "    def fit(self, graphs, y=None):\n",
    "        \"\"\"Learn WL vocabulary from training graphs\"\"\"\n",
    "        all_signatures_per_iteration = [set() for _ in range(self.wl_iterations + 1)]\n",
    "\n",
    "        for G in graphs:\n",
    "            graph_signatures = self._generate_wl_signatures(G)\n",
    "            for iteration in range(self.wl_iterations + 1):\n",
    "                all_signatures_per_iteration[iteration].update(graph_signatures[iteration])\n",
    "\n",
    "        self.vocab_per_iteration_ = []\n",
    "        self.vocab_size_per_iteration_ = []\n",
    "\n",
    "        for iteration in range(self.wl_iterations + 1):\n",
    "            signatures = sorted(all_signatures_per_iteration[iteration])\n",
    "            vocab = {sig: idx for idx, sig in enumerate(signatures)}\n",
    "            self.vocab_per_iteration_.append(vocab)\n",
    "            self.vocab_size_per_iteration_.append(len(vocab))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, graphs):\n",
    "        \"\"\"\n",
    "        Transform graphs to feature matrix.\n",
    "\n",
    "        Feature composition:\n",
    "        1. Chemical Laplacian eigenvalues (n_eigen + 2)\n",
    "        2. Topological features (15)\n",
    "        3. Bridge tree features (15)\n",
    "        4. WL Histograms (vocabulary-dependent)\n",
    "        \"\"\"\n",
    "        features_list = []\n",
    "\n",
    "        for G in graphs:\n",
    "            # Extract node and edge labels\n",
    "            node_labels = {n: G.nodes[n]['atom_type'] for n in G.nodes()}\n",
    "            edge_labels = {(u, v): G.edges[u, v].get('bond_order', 1.0)\n",
    "                          for u, v in G.edges()}\n",
    "\n",
    "            # 1. Chemical Laplacian features\n",
    "            chem_laplacian = self._extract_chemical_laplacian_features(G, node_labels, edge_labels)\n",
    "\n",
    "            # 2. Topological features\n",
    "            topology = self._extract_topological_features(G)\n",
    "\n",
    "            # 3. Bridge tree features\n",
    "            bridge_tree = self._extract_bridge_tree_features(G)\n",
    "\n",
    "            # 4. WL histograms\n",
    "            wl_hist = self._compute_wl_histogram(G)\n",
    "\n",
    "            # Concatenate all features\n",
    "            features = np.concatenate([chem_laplacian, topology, bridge_tree, wl_hist])\n",
    "            features_list.append(features)\n",
    "\n",
    "        return np.array(features_list)\n",
    "\n",
    "\n",
    "def pyg_to_networkx(data, atom_types, bond_types):\n",
    "    \"\"\"Convert PyTorch Geometric Data object to NetworkX graph\"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes with atom_type attribute\n",
    "    num_nodes = data.x.size(0)\n",
    "    for i in range(num_nodes):\n",
    "        atom_type_code = data.x[i].argmax().item()  # Get atom type from one-hot encoding\n",
    "        atom_type = atom_types.get(atom_type_code, 'C')\n",
    "        G.add_node(i, atom_type=atom_type)\n",
    "\n",
    "    # Add edges with bond_order attribute\n",
    "    edge_index = data.edge_index.numpy()\n",
    "    if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "        edge_attr = data.edge_attr.numpy()\n",
    "        for idx in range(edge_index.shape[1]):\n",
    "            src, dst = edge_index[0, idx], edge_index[1, idx]\n",
    "            if src < dst:  # Add edge only once (undirected)\n",
    "                bond_type_code = edge_attr[idx].argmax() if len(edge_attr[idx].shape) > 0 else int(edge_attr[idx])\n",
    "                bond_order = bond_types.get(bond_type_code, 1.0)\n",
    "                G.add_edge(int(src), int(dst), bond_order=bond_order)\n",
    "    else:\n",
    "        # No edge attributes, assume single bonds\n",
    "        for idx in range(edge_index.shape[1]):\n",
    "            src, dst = edge_index[0, idx], edge_index[1, idx]\n",
    "            if src < dst:\n",
    "                G.add_edge(int(src), int(dst), bond_order=1.0)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def process_dataset(dataset, dataset_name):\n",
    "    \"\"\"Process a single dataset with 10-fold CV\"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROCESSING: {dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get dataset-specific mappings\n",
    "    extractor_temp = MolecularGraphFeatureExtractor(dataset_name=dataset_name)\n",
    "    atom_types = extractor_temp.ATOM_TYPES\n",
    "    bond_types = extractor_temp.BOND_TYPES\n",
    "    \n",
    "    # Convert all PyG graphs to NetworkX\n",
    "    print(f\"Converting {len(dataset)} PyG graphs to NetworkX...\")\n",
    "    graphs = []\n",
    "    labels = []\n",
    "    for data in dataset:\n",
    "        G = pyg_to_networkx(data, atom_types, bond_types)\n",
    "        graphs.append(G)\n",
    "        labels.append(data.y.item())\n",
    "\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Convert labels if needed\n",
    "    if np.min(y) < 0:\n",
    "        y = ((y + 1) / 2).astype(int)\n",
    "    \n",
    "    print(f\"Total samples: {len(y)}\")\n",
    "    print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "    # 10-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    test_accs = []\n",
    "    best_params_per_fold = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(graphs, y), 1):\n",
    "        # Split graphs and labels\n",
    "        train_graphs = [graphs[i] for i in train_idx]\n",
    "        test_graphs = [graphs[i] for i in test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "\n",
    "        # Fit feature extractor on training data ONLY\n",
    "        extractor = MolecularGraphFeatureExtractor(n_eigen=10, wl_iterations=3, dataset_name=dataset_name)\n",
    "        extractor.fit(train_graphs)\n",
    "\n",
    "        # Transform both train and test\n",
    "        X_train = extractor.transform(train_graphs)\n",
    "        X_test = extractor.transform(test_graphs)\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Grid search for best hyperparameters\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "            'kernel': ['rbf']\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            SVC(random_state=42),\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        best_params_per_fold.append(grid_search.best_params_)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        best_svm = grid_search.best_estimator_\n",
    "        test_acc = best_svm.score(X_test_scaled, y_test)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f\"Fold {fold}/10: Test Acc = {test_acc:.3f}\")\n",
    "\n",
    "    # Find most common hyperparameters\n",
    "    c_values = [p['C'] for p in best_params_per_fold]\n",
    "    gamma_values = [p['gamma'] for p in best_params_per_fold]\n",
    "    \n",
    "    best_c = Counter(c_values).most_common(1)[0][0]\n",
    "    best_gamma = Counter(gamma_values).most_common(1)[0][0]\n",
    "    \n",
    "    mean_acc = np.mean(test_accs)\n",
    "    std_acc = np.std(test_accs)\n",
    "    \n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'mean_test_acc': mean_acc,\n",
    "        'std_test_acc': std_acc,\n",
    "        'best_C': best_c,\n",
    "        'best_gamma': best_gamma\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from torch_geometric.datasets import TUDataset\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"QUANTUM HACKATHON: MOLECULAR CLASSIFICATION (3 Datasets)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n[1] LOADING DATASETS...\")\n",
    "    \n",
    "    datasets = []\n",
    "    names = [\"MUTAG\", \"PTC_MR\", \"AIDS\"]\n",
    "    \n",
    "    for NAME in names:\n",
    "        try:\n",
    "            dataset = TUDataset(root=f'/tmp/{NAME}', name=NAME)\n",
    "            datasets.append(dataset)\n",
    "            print(f\"✓ Dataset {NAME} loaded successfully\")\n",
    "            print(f\"  - Number of graphs: {len(dataset)}\")\n",
    "            print(f\"  - Number of classes: {dataset.num_classes}\")\n",
    "            print(f\"  - Number of node features: {dataset.num_node_features}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading dataset {NAME}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Process each dataset\n",
    "    results = []\n",
    "    for dataset, name in zip(datasets, names):\n",
    "        result = process_dataset(dataset, name)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Dataset':<15} {'Mean Test Acc':<20} {'Best C':<10} {'Best Gamma'}\")\n",
    "    print(\"-\"*70)\n",
    "    for r in results:\n",
    "        print(f\"{r['dataset']:<15} {r['mean_test_acc']:.3f} ± {r['std_test_acc']:.3f}         \"\n",
    "              f\"{r['best_C']:<10} {r['best_gamma']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b3711-834a-4575-98b3-f11b73676904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
